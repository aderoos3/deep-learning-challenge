# deep-learning-challenge
## Analysis Report of Previous Alphabet Soup Recipients

	The goal of this analysis is to help Alphabet Soup find a deep learning model to select applicants to receive funding from them. The deep learning model used data from 34,000 previous organizations that have tried to obtain funding from them in the past. The data included the type of industry, the ask amount, the income amount, and if the money was used effectively among other variables. 
	I started by creating one model and then tried three more attempts to improve the accuracy rate. This would ensure that the model will give Alphabet Soup the best results of funding the right potential clients. 
	This analysis had preprocessing of the data and creating the model. For preprocessing the data, I did a few things to get a better sense of the data. I got the unique values, dropped unnecessary columns, and binned the data. The target variable ended up being “is successful” which was measuring how successful an organization was with their funds. The featured variable were everything else. This included application type, affiliation, classification, use case, organization, status, income amount, special considerations, and ask amount. I removed the name and EIN from the data. It wasn’t necessary and that data wouldn’t do anything to do the model. Those data points are better for identifying the data. 
	I had to go through several trials to attempt to get a good accuracy rate and I never quite got to 75%. The first model I used two hidden layers, relu functions for the hidden layers, and 8 and 6 neurons in the first and second layer respectively. This set-up was similar to other set-ups done in class so I felt like it was a good starting point. I was at about 73% accuracy. My next steps were to optimize my model.
	I tried a variety of things to improve the optimization. Overall, I added more bins to one of the binned values for each attempt. Each attempt has an extra bin in the c-values. For the first attempt, I just tried adding more neurons to the second layer. That is a strategy that has worked in the past. This gave me 73%. For the second attempt, I went back to 6 neurons in the second layer and changed the activation function in the first layer to sigmoid. In previous class examples, changing the activation function has also shown success. I remember from playing with the tensor flow playground that sometimes too many neurons doesn’t make a difference. The second attempt was still only 71% accurate. Finally, I kept the sigmoid function in the first layer, added two more neurons in the second layer and added a third layer with 4 neurons. This gave me 71% accuracy. None of the new models improved my accuracy. I was trying to try a variety of things since I wasn’t find success with anything in particular. Sometimes I think changing too many things can also be bad because you can’t track what is working and what isn’t. That’s why I only really focused on the binning, activation functions, and number of neurons. 
	I’ve described the number for each of the models and none of them were quite 75%, but they were all close. I think trying a different model would be helpful. I would recommend staying with 3 layers, but adding more neurons. Probably at least 10 in each layer. That will built the connections and could lead to a better model. There is data from 34,000 organizations so that is a lot to process. 
![image](https://github.com/aderoos3/deep-learning-challenge/assets/129622863/a2bf5884-f10c-40d6-a00e-ccdae51eb168)
