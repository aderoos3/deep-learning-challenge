# deep-learning-challenge
## Analysis Report of Previous Alphabet Soup Recipients

The goal of this analysis is to help Alphabet Soup find a deep learning model to select applicants to receive funding from them. The deep learning model used data from 34,000 previous organizations that have tried to obtain funding from them in the past. The data included the type of industry, the ask amount, the income amount, and if the money was used effectively among other variables. I started by creating one model and then tried three more attempts to improve the accuracy rate. This would ensure that the model will give Alphabet Soup the best results of funding the right potential clients. 

This analysis had preprocessing of the data and creating the model. For preprocessing the data, I did a few things to get a better sense of the data. I got the unique values, dropped unnecessary columns, and binned the data. The target variable ended up being “is successful” which was measuring how successful an organization was with their funds. The featured variables were everything else. This included application type, affiliation, classification, use case, organization, status, income amount, special considerations, and ask amount. I removed the name and EIN from the data. It wasn’t necessary and that data wouldn’t do anything to do the model. Those data points are better for identifying the data. 
	
I had to go through several trials to attempt to get a good accuracy rate and I never quite got to 75%. The first model I used two hidden layers with 8 neurons in the first and 6 neurons in the second, and relu functions for both hidden layers. The output layer was a sigmoid function. This set-up was similar to other set-ups done in class so I felt like it was a good starting point. I was at about 73% accuracy. My next steps were to optimize my model.
	
I tried a variety of things to improve the optimization. Overall, I added more bins to one of the binned values for each attempt. Each attempt has an extra bin in the c-values. For the first attempt, I just tried adding more neurons to the second layer. That is a strategy that has worked in the past. This gave me 73%. For the second attempt, I went back to 6 neurons in the second layer and changed the activation function in the first layer to sigmoid. In previous class examples, changing the activation function has also shown success. I wanted to see what only changing the activation function would do. The second attempt was still also 73% accurate. Finally, I kept the sigmoid function in the first layer and added two more neurons, added two more neurons in the second layer and added a third layer with 8 neurons. I decided to just try combining more neurons and the activation function change. This gave me 72% accuracy. That was actually lower than before. None of the new models improved my accuracy. I was trying a few specific changes since I wasn’t find success with anything in particular. I did't want to change too many things at once until I knew how a change coud affect the mode.
	
I never quite got a 75% activation rate. So Alphabet Soup should definitely try another model. I would recommend staying with 3 layers, but adding more neurons. Probably at least 10 in each layer. That will built the connections and could lead to a better model. There is data from 34,000 organizations so that is a lot to process. More neurons can find more connections between all that data and could train a better model. 
